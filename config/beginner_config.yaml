# 初心者向け学習設定ファイル
#
# このファイルは、AI学習が初めての方でも使いやすいように
# 安全で理解しやすいパラメータに設定されています。
#
# 使い方:
#   python src/train.py --config config/beginner_config.yaml

# =====================================
# ランダムシード（再現性のため）
# =====================================
# 同じ結果を再現できるように固定値を設定
seed: 42

# =====================================
# モデル設定
# =====================================
model:
  # 使用するモデル名
  # 初心者におすすめ: DistilBERT（BERTの軽量・高速版）
  name: "distilbert-base-uncased"

  # タスクの種類
  # - sequence_classification: テキスト分類（感情分析など）
  # - token_classification: 単語分類（固有表現認識など）
  # - qa: 質問応答
  task_type: "sequence_classification"

  # ラベル（分類クラス）の数
  # 2 = ポジティブ/ネガティブなど2つのクラス
  num_labels: 2

  # 入力テキストの最大長（トークン数）
  # 256 = 初心者向けに短めに設定（速度重視）
  # 512 = より長い文章を扱いたい場合
  max_length: 256

  # 事前学習済みモデルを使用するか
  # true = 推奨（少ないデータで高精度）
  pretrained: true

  # モデルの詳細設定（オプション）
  # 通常は変更不要
  config_overrides:
    # ドロップアウト率（過学習防止）
    # dropout: 0.1
    # attention_dropout: 0.1

# =====================================
# データセット設定
# =====================================
dataset:
  # Hugging Face Hubのデータセット名
  # 初心者向けおすすめ:
  # - "imdb": 映画レビュー（感情分析）
  # - "sst2": 短文の感情分析
  # - "ag_news": ニュース分類
  name: "imdb"

  # データセットのサブセット名（必要な場合のみ）
  subset: null

  # 【重要】初心者向けに少量のデータで試す場合
  # この設定を有効にしてください（学習時間が大幅に短縮）
  # カスタム設定例:
  # custom_split:
  #   train: "train[:1000]"  # 訓練データ1000件のみ
  #   test: "test[:200]"     # テストデータ200件のみ

# =====================================
# 学習設定
# =====================================
training:
  # モデルの保存先
  output_dir: "./models/beginner-model"

  # エポック数（全データを何回学習するか）
  # 初心者向け推奨: 3
  # より高精度を目指す場合: 5
  # 注意: 多すぎると過学習のリスク
  num_epochs: 3

  # 訓練時のバッチサイズ（一度に処理するデータ数）
  # メモリ少なめ → 8
  # メモリ十分 → 16
  # GPU使用時 → 16 or 32
  batch_size: 8

  # 評価時のバッチサイズ
  # 訓練より大きくてもOK（推論のみなのでメモリ節約）
  eval_batch_size: 16

  # 学習率（どれだけ大きく更新するか）
  # 一般的な値: 2e-5 (0.00002)
  # 調整が必要な場合: 1e-5 〜 5e-5 の範囲で試す
  learning_rate: 2.0e-5

  # 重み減衰（過学習を防ぐ正則化パラメータ）
  weight_decay: 0.01

  # ウォームアップステップ数
  # 学習開始時に学習率を徐々に上げる
  warmup_steps: 500

  # ログの保存先
  logging_dir: "./logs"

  # 何ステップごとにログを出力するか
  logging_steps: 50

  # 保存するチェックポイントの最大数
  # 古いものから削除される（ディスク容量節約）
  save_total_limit: 2

  # 混合精度学習（GPUがある場合のみ有効）
  # true = 高速化、メモリ節約
  # false = CPU使用時、または互換性問題がある場合
  fp16: false  # 初心者は false を推奨

  # 学習の進捗をどこに記録するか
  # オプション: tensorboard, wandb, none
  report_to: ["tensorboard"]

  # 早期停止（精度が改善しなくなったら自動停止）
  early_stopping: true

  # 早期停止の忍耐力（何エポック改善なしで停止するか）
  early_stopping_patience: 3

# =====================================
# データ処理設定
# =====================================
# データ前処理の並列処理数
# CPUコア数に応じて調整
# 1 = 安全（問題が起きにくい）
# 4 = 推奨（バランス良い）
# 8+ = 高速（多コアCPUの場合）
num_proc: 4

# =====================================
# 評価設定
# =====================================
evaluation:
  # 主要な評価メトリクス
  metric: "accuracy"

  # その他の追加メトリクス（オプション）
  # - f1: F1スコア（不均衡データに有効）
  # - precision: 精度
  # - recall: 再現率

# =====================================
# 初心者向けヒント
# =====================================
#
# 【最初に試すこと】
# 1. このファイルをそのまま使って学習してみる
#    python src/train.py --config config/beginner_config.yaml
#
# 2. TensorBoardで進捗を確認
#    tensorboard --logdir ./logs
#
# 【よくある調整】
# ■ メモリ不足エラーが出たら:
#   - batch_size: 8 → 4
#   - max_length: 256 → 128
#
# ■ もっと精度を上げたい:
#   - num_epochs: 3 → 5
#   - データ量を増やす
#
# ■ 学習が不安定:
#   - learning_rate: 2e-5 → 1e-5
#
# ■ 学習を速くしたい:
#   - model.name を "distilbert" のまま
#   - batch_size を増やす（メモリが許せば）
#   - fp16: true（GPUがあれば）
#
# 【日本語を扱いたい場合】
# model:
#   name: "cl-tohoku/bert-base-japanese"
# dataset:
#   name: "日本語のデータセット名"
#
# =====================================
