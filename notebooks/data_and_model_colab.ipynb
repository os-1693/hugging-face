{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データセット処理とモデル構築\n",
    "\n",
    "Hugging Face Hubからデータセットを読み込み、前処理を行い、モデルを構築します。\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 環境設定\n",
    "\n",
    "まず、GPUが有効になっているか確認します。\n",
    "\n",
    "**設定方法**: メニュー → ランタイム → ランタイムのタイプを変更 → GPU を選択"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU確認\n",
    "import torch\n",
    "print(f\"GPU利用可能: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU名: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ライブラリのインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Dict, Any\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    PreTrainedTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoConfig,\n",
    "    PreTrainedModel\n",
    ")\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. DatasetLoaderクラスの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetLoader:\n",
    "    \"\"\"\n",
    "    Hugging Face データセットのローダークラス\n",
    "\n",
    "    Hugging Face Hubからデータセットをダウンロードし、\n",
    "    トークナイズなどの前処理を行います。\n",
    "\n",
    "    Attributes:\n",
    "        dataset_name (str): データセット名\n",
    "        tokenizer (PreTrainedTokenizer): トークナイザー\n",
    "        max_length (int): 最大シーケンス長\n",
    "        subset (str): データセットのサブセット名\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_name: str,\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        max_length: int = 512,\n",
    "        subset: Optional[str] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset_name: Hugging Face Hubのデータセット名\n",
    "            tokenizer: トークナイザー\n",
    "            max_length: 最大シーケンス長\n",
    "            subset: データセットのサブセット名\n",
    "        \"\"\"\n",
    "        self.dataset_name = dataset_name\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.subset = subset\n",
    "\n",
    "    def load(self) -> DatasetDict:\n",
    "        \"\"\"\n",
    "        データセットを読み込む\n",
    "\n",
    "        Hugging Face Hubからデータセットをダウンロードします。\n",
    "        初回はインターネット接続が必要で、キャッシュに保存されます。\n",
    "\n",
    "        Returns:\n",
    "            DatasetDict: 読み込んだデータセット（train, test等を含む）\n",
    "\n",
    "        Raises:\n",
    "            ValueError: データセットが見つからない場合\n",
    "        \"\"\"\n",
    "        logger.info(f\"Loading dataset: {self.dataset_name}\")\n",
    "\n",
    "        if self.subset:\n",
    "            dataset = load_dataset(self.dataset_name, self.subset)\n",
    "        else:\n",
    "            dataset = load_dataset(self.dataset_name)\n",
    "\n",
    "        logger.info(f\"Dataset loaded: {dataset}\")\n",
    "        return dataset\n",
    "\n",
    "    def preprocess_function(self, examples: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        データの前処理（テキスト分類タスク用の例）\n",
    "\n",
    "        Args:\n",
    "            examples: バッチデータ\n",
    "\n",
    "        Returns:\n",
    "            トークナイズされたデータ\n",
    "        \"\"\"\n",
    "        # テキストカラムを自動検出\n",
    "        text_columns = [\"text\", \"sentence\", \"content\", \"review\", \"description\", \"question\"]\n",
    "        text_column = None\n",
    "        for col in text_columns:\n",
    "            if col in examples:\n",
    "                text_column = col\n",
    "                break\n",
    "\n",
    "        if text_column is None:\n",
    "            available_cols = list(examples.keys())\n",
    "            raise ValueError(\n",
    "                f\"テキストカラムが見つかりません。\"\n",
    "                f\"期待されるカラム: {text_columns}, \"\n",
    "                f\"利用可能なカラム: {available_cols}\"\n",
    "            )\n",
    "\n",
    "        return self.tokenizer(\n",
    "            examples[text_column],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "        )\n",
    "\n",
    "    def prepare_dataset(\n",
    "        self,\n",
    "        dataset: DatasetDict,\n",
    "        num_proc: int = 4,\n",
    "        remove_columns: Optional[list] = None\n",
    "    ) -> DatasetDict:\n",
    "        \"\"\"\n",
    "        データセットの前処理を適用\n",
    "\n",
    "        Args:\n",
    "            dataset: 処理対象のデータセット\n",
    "            num_proc: 並列処理数\n",
    "            remove_columns: 削除するカラム名のリスト\n",
    "\n",
    "        Returns:\n",
    "            前処理済みデータセット\n",
    "        \"\"\"\n",
    "        logger.info(\"Preprocessing dataset...\")\n",
    "\n",
    "        # デフォルトで削除するカラムを設定\n",
    "        if remove_columns is None and \"train\" in dataset:\n",
    "            remove_columns = [\n",
    "                col for col in dataset[\"train\"].column_names\n",
    "                if col not in [\"label\", \"labels\"]\n",
    "            ]\n",
    "\n",
    "        processed_dataset = dataset.map(\n",
    "            self.preprocess_function,\n",
    "            batched=True,\n",
    "            num_proc=num_proc,\n",
    "            remove_columns=remove_columns,\n",
    "        )\n",
    "\n",
    "        logger.info(\"Dataset preprocessing completed\")\n",
    "        return processed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. CustomDatasetBuilderクラスの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDatasetBuilder:\n",
    "    \"\"\"カスタムデータセットビルダー\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def from_csv(\n",
    "        train_path: str,\n",
    "        validation_path: Optional[str] = None,\n",
    "        test_path: Optional[str] = None\n",
    "    ) -> DatasetDict:\n",
    "        \"\"\"\n",
    "        CSVファイルからデータセットを作成\n",
    "\n",
    "        Args:\n",
    "            train_path: 訓練データのパス\n",
    "            validation_path: 検証データのパス\n",
    "            test_path: テストデータのパス\n",
    "\n",
    "        Returns:\n",
    "            DatasetDict\n",
    "        \"\"\"\n",
    "        datasets = {}\n",
    "\n",
    "        datasets[\"train\"] = Dataset.from_csv(train_path)\n",
    "\n",
    "        if validation_path:\n",
    "            datasets[\"validation\"] = Dataset.from_csv(validation_path)\n",
    "\n",
    "        if test_path:\n",
    "            datasets[\"test\"] = Dataset.from_csv(test_path)\n",
    "\n",
    "        return DatasetDict(datasets)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_json(\n",
    "        train_path: str,\n",
    "        validation_path: Optional[str] = None,\n",
    "        test_path: Optional[str] = None\n",
    "    ) -> DatasetDict:\n",
    "        \"\"\"\n",
    "        JSONファイルからデータセットを作成\n",
    "\n",
    "        Args:\n",
    "            train_path: 訓練データのパス\n",
    "            validation_path: 検証データのパス\n",
    "            test_path: テストデータのパス\n",
    "\n",
    "        Returns:\n",
    "            DatasetDict\n",
    "        \"\"\"\n",
    "        datasets = {}\n",
    "\n",
    "        datasets[\"train\"] = Dataset.from_json(train_path)\n",
    "\n",
    "        if validation_path:\n",
    "            datasets[\"validation\"] = Dataset.from_json(validation_path)\n",
    "\n",
    "        if test_path:\n",
    "            datasets[\"test\"] = Dataset.from_json(test_path)\n",
    "\n",
    "        return DatasetDict(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ModelBuilderクラスの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelBuilder:\n",
    "    \"\"\"Hugging Faceモデルのビルダークラス\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def build_sequence_classification_model(\n",
    "        model_name: str,\n",
    "        num_labels: int,\n",
    "        pretrained: bool = True,\n",
    "        config_overrides: Optional[dict] = None\n",
    "    ) -> PreTrainedModel:\n",
    "        \"\"\"\n",
    "        テキスト分類モデルを構築\n",
    "\n",
    "        Args:\n",
    "            model_name: モデル名（例: \"bert-base-uncased\"）\n",
    "            num_labels: ラベル数\n",
    "            pretrained: 事前学習済みモデルを使用するか\n",
    "            config_overrides: 設定のオーバーライド\n",
    "\n",
    "        Returns:\n",
    "            分類モデル\n",
    "        \"\"\"\n",
    "        logger.info(f\"Building sequence classification model: {model_name}\")\n",
    "\n",
    "        try:\n",
    "            config = AutoConfig.from_pretrained(model_name)\n",
    "        except OSError as e:\n",
    "            logger.error(f\"Model '{model_name}' not found\")\n",
    "            logger.info(\"Available models: https://huggingface.co/models\")\n",
    "            raise ValueError(f\"Invalid model name: {model_name}\") from e\n",
    "\n",
    "        config.num_labels = num_labels\n",
    "\n",
    "        if config_overrides:\n",
    "            for key, value in config_overrides.items():\n",
    "                setattr(config, key, value)\n",
    "\n",
    "        try:\n",
    "            if pretrained:\n",
    "                model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                    model_name,\n",
    "                    config=config,\n",
    "                    ignore_mismatched_sizes=True\n",
    "                )\n",
    "            else:\n",
    "                model = AutoModelForSequenceClassification.from_config(config)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load model: {e}\")\n",
    "            raise\n",
    "\n",
    "        logger.info(f\"Model built successfully with {num_labels} labels\")\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def build_token_classification_model(\n",
    "        model_name: str,\n",
    "        num_labels: int,\n",
    "        pretrained: bool = True,\n",
    "        config_overrides: Optional[dict] = None\n",
    "    ) -> PreTrainedModel:\n",
    "        \"\"\"\n",
    "        トークン分類モデルを構築（NERなど）\n",
    "\n",
    "        Args:\n",
    "            model_name: モデル名\n",
    "            num_labels: ラベル数\n",
    "            pretrained: 事前学習済みモデルを使用するか\n",
    "            config_overrides: 設定のオーバーライド\n",
    "\n",
    "        Returns:\n",
    "            トークン分類モデル\n",
    "        \"\"\"\n",
    "        logger.info(f\"Building token classification model: {model_name}\")\n",
    "\n",
    "        try:\n",
    "            config = AutoConfig.from_pretrained(model_name)\n",
    "        except OSError as e:\n",
    "            logger.error(f\"Model '{model_name}' not found\")\n",
    "            logger.info(\"Available models: https://huggingface.co/models\")\n",
    "            raise ValueError(f\"Invalid model name: {model_name}\") from e\n",
    "\n",
    "        config.num_labels = num_labels\n",
    "\n",
    "        if config_overrides:\n",
    "            for key, value in config_overrides.items():\n",
    "                setattr(config, key, value)\n",
    "\n",
    "        try:\n",
    "            if pretrained:\n",
    "                model = AutoModelForTokenClassification.from_pretrained(\n",
    "                    model_name,\n",
    "                    config=config,\n",
    "                    ignore_mismatched_sizes=True\n",
    "                )\n",
    "            else:\n",
    "                model = AutoModelForTokenClassification.from_config(config)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load model: {e}\")\n",
    "            raise\n",
    "\n",
    "        logger.info(\"Token classification model built successfully\")\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def build_qa_model(\n",
    "        model_name: str,\n",
    "        pretrained: bool = True,\n",
    "        config_overrides: Optional[dict] = None\n",
    "    ) -> PreTrainedModel:\n",
    "        \"\"\"\n",
    "        質問応答モデルを構築\n",
    "\n",
    "        Args:\n",
    "            model_name: モデル名\n",
    "            pretrained: 事前学習済みモデルを使用するか\n",
    "            config_overrides: 設定のオーバーライド\n",
    "\n",
    "        Returns:\n",
    "            質問応答モデル\n",
    "        \"\"\"\n",
    "        logger.info(f\"Building QA model: {model_name}\")\n",
    "\n",
    "        try:\n",
    "            config = AutoConfig.from_pretrained(model_name)\n",
    "        except OSError as e:\n",
    "            logger.error(f\"Model '{model_name}' not found\")\n",
    "            logger.info(\"Available models: https://huggingface.co/models\")\n",
    "            raise ValueError(f\"Invalid model name: {model_name}\") from e\n",
    "\n",
    "        if config_overrides:\n",
    "            for key, value in config_overrides.items():\n",
    "                setattr(config, key, value)\n",
    "\n",
    "        try:\n",
    "            if pretrained:\n",
    "                model = AutoModelForQuestionAnswering.from_pretrained(\n",
    "                    model_name,\n",
    "                    config=config,\n",
    "                    ignore_mismatched_sizes=True\n",
    "                )\n",
    "            else:\n",
    "                model = AutoModelForQuestionAnswering.from_config(config)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load model: {e}\")\n",
    "            raise\n",
    "\n",
    "        logger.info(\"QA model built successfully\")\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def build_causal_lm_model(\n",
    "        model_name: str,\n",
    "        pretrained: bool = True,\n",
    "        config_overrides: Optional[dict] = None\n",
    "    ) -> PreTrainedModel:\n",
    "        \"\"\"\n",
    "        因果言語モデルを構築（GPTなど）\n",
    "\n",
    "        Args:\n",
    "            model_name: モデル名\n",
    "            pretrained: 事前学習済みモデルを使用するか\n",
    "            config_overrides: 設定のオーバーライド\n",
    "\n",
    "        Returns:\n",
    "            因果言語モデル\n",
    "        \"\"\"\n",
    "        logger.info(f\"Building Causal LM model: {model_name}\")\n",
    "\n",
    "        try:\n",
    "            config = AutoConfig.from_pretrained(model_name)\n",
    "        except OSError as e:\n",
    "            logger.error(f\"Model '{model_name}' not found\")\n",
    "            logger.info(\"Available models: https://huggingface.co/models\")\n",
    "            raise ValueError(f\"Invalid model name: {model_name}\") from e\n",
    "\n",
    "        if config_overrides:\n",
    "            for key, value in config_overrides.items():\n",
    "                setattr(config, key, value)\n",
    "\n",
    "        try:\n",
    "            if pretrained:\n",
    "                model = AutoModelForCausalLM.from_pretrained(\n",
    "                    model_name,\n",
    "                    config=config\n",
    "                )\n",
    "            else:\n",
    "                model = AutoModelForCausalLM.from_config(config)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load model: {e}\")\n",
    "            raise\n",
    "\n",
    "        logger.info(\"Causal LM model built successfully\")\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 使用例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用例\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# トークナイザーの読み込み\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# DatasetLoaderの使用\n",
    "loader = DatasetLoader(\"imdb\", tokenizer)\n",
    "dataset = loader.load()\n",
    "print(f\"読み込んだデータセット: {dataset}\")\n",
    "\n",
    "# 前処理\n",
    "processed_dataset = loader.prepare_dataset(dataset)\n",
    "print(f\"前処理済みデータセット: {processed_dataset}\")\n",
    "\n",
    "# ModelBuilderの使用\n",
    "model = ModelBuilder.build_sequence_classification_model(\n",
    "    model_name=\"distilbert-base-uncased\",\n",
    "    num_labels=2,\n",
    "    pretrained=True\n",
    ")\n",
    "print(f\"モデル: distilbert-base-uncased\")\n",
    "print(f\"パラメータ数: {model.num_parameters():,}\")\n",
    "print(f\"ラベル数: 2\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}