{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# データセット処理モジュール\n",
                "\n",
                "Hugging Face Hubからデータセットを読み込み、前処理を行うための\n",
                "ユーティリティクラスを提供します。\n",
                "\n",
                "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. 環境設定\n",
                "\n",
                "まず、GPUが有効になっているか確認します。\n",
                "\n",
                "**設定方法**: メニュー → ランタイム → ランタイムのタイプを変更 → GPU を選択"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# GPU確認\n",
                "import torch\n",
                "print(f\"GPU利用可能: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU名: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. ライブラリのインストール"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q transformers datasets"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. ライブラリのインポート"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from typing import Optional, Dict, Any\n",
                "from datasets import load_dataset, Dataset, DatasetDict\n",
                "from transformers import PreTrainedTokenizer\n",
                "import logging\n",
                "\n",
                "logging.basicConfig(level=logging.INFO)\n",
                "logger = logging.getLogger(__name__)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. DatasetLoaderクラスの定義"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class DatasetLoader:\n",
                "    \"\"\"\n",
                "    Hugging Face データセットのローダークラス\n",
                "\n",
                "    Hugging Face Hubからデータセットをダウンロードし、\n",
                "    トークナイズなどの前処理を行います。\n",
                "\n",
                "    Attributes:\n",
                "        dataset_name (str): データセット名\n",
                "        tokenizer (PreTrainedTokenizer): トークナイザー\n",
                "        max_length (int): 最大シーケンス長\n",
                "        subset (str): データセットのサブセット名\n",
                "    \"\"\"\n",
                "\n",
                "    def __init__(\n",
                "        self,\n",
                "        dataset_name: str,\n",
                "        tokenizer: PreTrainedTokenizer,\n",
                "        max_length: int = 512,\n",
                "        subset: Optional[str] = None\n",
                "    ):\n",
                "        \"\"\"\n",
                "        Args:\n",
                "            dataset_name: Hugging Face Hubのデータセット名\n",
                "            tokenizer: トークナイザー\n",
                "            max_length: 最大シーケンス長\n",
                "            subset: データセットのサブセット名\n",
                "        \"\"\"\n",
                "        self.dataset_name = dataset_name\n",
                "        self.tokenizer = tokenizer\n",
                "        self.max_length = max_length\n",
                "        self.subset = subset\n",
                "\n",
                "    def load(self) -> DatasetDict:\n",
                "        \"\"\"\n",
                "        データセットを読み込む\n",
                "\n",
                "        Hugging Face Hubからデータセットをダウンロードします。\n",
                "        初回はインターネット接続が必要で、キャッシュに保存されます。\n",
                "\n",
                "        Returns:\n",
                "            DatasetDict: 読み込んだデータセット（train, test等を含む）\n",
                "\n",
                "        Raises:\n",
                "            ValueError: データセットが見つからない場合\n",
                "        \"\"\"\n",
                "        logger.info(f\"Loading dataset: {self.dataset_name}\")\n",
                "\n",
                "        if self.subset:\n",
                "            dataset = load_dataset(self.dataset_name, self.subset)\n",
                "        else:\n",
                "            dataset = load_dataset(self.dataset_name)\n",
                "\n",
                "        logger.info(f\"Dataset loaded: {dataset}\")\n",
                "        return dataset\n",
                "\n",
                "    def preprocess_function(self, examples: Dict[str, Any]) -> Dict[str, Any]:\n",
                "        \"\"\"\n",
                "        データの前処理（テキスト分類タスク用の例）\n",
                "\n",
                "        Args:\n",
                "            examples: バッチデータ\n",
                "\n",
                "        Returns:\n",
                "            トークナイズされたデータ\n",
                "        \"\"\"\n",
                "        # テキストカラムを自動検出\n",
                "        text_columns = [\"text\", \"sentence\", \"content\", \"review\", \"description\", \"question\"]\n",
                "        text_column = None\n",
                "        for col in text_columns:\n",
                "            if col in examples:\n",
                "                text_column = col\n",
                "                break\n",
                "\n",
                "        if text_column is None:\n",
                "            available_cols = list(examples.keys())\n",
                "            raise ValueError(\n",
                "                f\"テキストカラムが見つかりません。\"\n",
                "                f\"期待されるカラム: {text_columns}, \"\n",
                "                f\"利用可能なカラム: {available_cols}\"\n",
                "            )\n",
                "\n",
                "        return self.tokenizer(\n",
                "            examples[text_column],\n",
                "            padding=\"max_length\",\n",
                "            truncation=True,\n",
                "            max_length=self.max_length,\n",
                "        )\n",
                "\n",
                "    def prepare_dataset(\n",
                "        self,\n",
                "        dataset: DatasetDict,\n",
                "        num_proc: int = 4,\n",
                "        remove_columns: Optional[list] = None\n",
                "    ) -> DatasetDict:\n",
                "        \"\"\"\n",
                "        データセットの前処理を適用\n",
                "\n",
                "        Args:\n",
                "            dataset: 処理対象のデータセット\n",
                "            num_proc: 並列処理数\n",
                "            remove_columns: 削除するカラム名のリスト\n",
                "\n",
                "        Returns:\n",
                "            前処理済みデータセット\n",
                "        \"\"\"\n",
                "        logger.info(\"Preprocessing dataset...\")\n",
                "\n",
                "        # デフォルトで削除するカラムを設定\n",
                "        if remove_columns is None and \"train\" in dataset:\n",
                "            remove_columns = [\n",
                "                col for col in dataset[\"train\"].column_names\n",
                "                if col not in [\"label\", \"labels\"]\n",
                "            ]\n",
                "\n",
                "        processed_dataset = dataset.map(\n",
                "            self.preprocess_function,\n",
                "            batched=True,\n",
                "            num_proc=num_proc,\n",
                "            remove_columns=remove_columns,\n",
                "        )\n",
                "\n",
                "        logger.info(\"Dataset preprocessing completed\")\n",
                "        return processed_dataset"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. CustomDatasetBuilderクラスの定義"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class CustomDatasetBuilder:\n",
                "    \"\"\"カスタムデータセットビルダー\"\"\"\n",
                "\n",
                "    @staticmethod\n",
                "    def from_csv(\n",
                "        train_path: str,\n",
                "        validation_path: Optional[str] = None,\n",
                "        test_path: Optional[str] = None\n",
                "    ) -> DatasetDict:\n",
                "        \"\"\"\n",
                "        CSVファイルからデータセットを作成\n",
                "\n",
                "        Args:\n",
                "            train_path: 訓練データのパス\n",
                "            validation_path: 検証データのパス\n",
                "            test_path: テストデータのパス\n",
                "\n",
                "        Returns:\n",
                "            DatasetDict\n",
                "        \"\"\"\n",
                "        datasets = {}\n",
                "\n",
                "        datasets[\"train\"] = Dataset.from_csv(train_path)\n",
                "\n",
                "        if validation_path:\n",
                "            datasets[\"validation\"] = Dataset.from_csv(validation_path)\n",
                "\n",
                "        if test_path:\n",
                "            datasets[\"test\"] = Dataset.from_csv(test_path)\n",
                "\n",
                "        return DatasetDict(datasets)\n",
                "\n",
                "    @staticmethod\n",
                "    def from_json(\n",
                "        train_path: str,\n",
                "        validation_path: Optional[str] = None,\n",
                "        test_path: Optional[str] = None\n",
                "    ) -> DatasetDict:\n",
                "        \"\"\"\n",
                "        JSONファイルからデータセットを作成\n",
                "\n",
                "        Args:\n",
                "            train_path: 訓練データのパス\n",
                "            validation_path: 検証データのパス\n",
                "            test_path: テストデータのパス\n",
                "\n",
                "        Returns:\n",
                "            DatasetDict\n",
                "        \"\"\"\n",
                "        datasets = {}\n",
                "\n",
                "        datasets[\"train\"] = Dataset.from_json(train_path)\n",
                "\n",
                "        if validation_path:\n",
                "            datasets[\"validation\"] = Dataset.from_json(validation_path)\n",
                "\n",
                "        if test_path:\n",
                "            datasets[\"test\"] = Dataset.from_json(test_path)\n",
                "\n",
                "        return DatasetDict(datasets)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. 使用例"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 使用例\n",
                "from transformers import AutoTokenizer\n",
                "\n",
                "# トークナイザーの読み込み\n",
                "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
                "\n",
                "# DatasetLoaderの使用\n",
                "loader = DatasetLoader(\"imdb\", tokenizer)\n",
                "dataset = loader.load()\n",
                "print(f\"読み込んだデータセット: {dataset}\")\n",
                "\n",
                "# 前処理\n",
                "processed_dataset = loader.prepare_dataset(dataset)\n",
                "print(f\"前処理済みデータセット: {processed_dataset}\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
