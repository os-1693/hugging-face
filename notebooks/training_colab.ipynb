{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# モデル学習\n",
                "\n",
                "Hugging Face Transformersを使用してモデルを学習します。\n",
                "\n",
                "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. 環境設定\n",
                "\n",
                "まず、GPUが有効になっているか確認します。\n",
                "\n",
                "**設定方法**: メニュー → ランタイム → ランタイムのタイプを変更 → GPU を選択"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# GPU確認\n",
                "import torch\n",
                "print(f\"GPU利用可能: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU名: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. ライブラリのインストール"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q transformers datasets evaluate accelerate"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. ライブラリのインポート"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from typing import Optional, Dict, Any\n",
                "from datasets import DatasetDict\n",
                "from transformers import (\n",
                "    PreTrainedModel,\n",
                "    PreTrainedTokenizer,\n",
                "    TrainingArguments,\n",
                "    Trainer,\n",
                "    DataCollatorWithPadding,\n",
                "    EarlyStoppingCallback\n",
                ")\n",
                "import evaluate\n",
                "import numpy as np\n",
                "import logging\n",
                "\n",
                "logging.basicConfig(level=logging.INFO)\n",
                "logger = logging.getLogger(__name__)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. 学習関数の定義"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train(\n",
                "    model: PreTrainedModel,\n",
                "    tokenizer: PreTrainedTokenizer,\n",
                "    dataset: DatasetDict,\n",
                "    output_dir: str = \"./results\",\n",
                "    num_train_epochs: int = 3,\n",
                "    per_device_train_batch_size: int = 16,\n",
                "    per_device_eval_batch_size: int = 16,\n",
                "    warmup_steps: int = 500,\n",
                "    weight_decay: float = 0.01,\n",
                "    logging_dir: str = \"./logs\",\n",
                "    logging_steps: int = 10,\n",
                "    eval_strategy: str = \"epoch\",\n",
                "    save_strategy: str = \"epoch\",\n",
                "    load_best_model_at_end: bool = True,\n",
                "    metric_for_best_model: str = \"accuracy\",\n",
                "    greater_is_better: bool = True,\n",
                "    save_total_limit: int = 3,\n",
                "    fp16: bool = True,\n",
                "    gradient_accumulation_steps: int = 1,\n",
                "    learning_rate: float = 5e-5,\n",
                "    lr_scheduler_type: str = \"linear\",\n",
                "    seed: int = 42,\n",
                "    data_seed: int = 42,\n",
                "    **kwargs\n",
                ") -> Trainer:\n",
                "    \"\"\"\n",
                "    モデルを学習する関数\n",
                "\n",
                "    Args:\n",
                "        model: 学習対象のモデル\n",
                "        tokenizer: トークナイザー\n",
                "        dataset: 学習データセット（train, validationを含む）\n",
                "        output_dir: 出力ディレクトリ\n",
                "        num_train_epochs: エポック数\n",
                "        per_device_train_batch_size: デバイスごとの訓練バッチサイズ\n",
                "        per_device_eval_batch_size: デバイスごとの評価バッチサイズ\n",
                "        warmup_steps: ウォームアップステップ数\n",
                "        weight_decay: 重み減衰\n",
                "        logging_dir: ログディレクトリ\n",
                "        logging_steps: ログ出力ステップ間隔\n",
                "        eval_strategy: 評価戦略\n",
                "        save_strategy: 保存戦略\n",
                "        load_best_model_at_end: 学習終了時にベストモデルをロード\n",
                "        metric_for_best_model: ベストモデル判定のメトリクス\n",
                "        greater_is_better: メトリクスが大きい方が良いか\n",
                "        save_total_limit: 保存するチェックポイントの最大数\n",
                "        fp16: 半精度学習を使用\n",
                "        gradient_accumulation_steps: 勾配蓄積ステップ数\n",
                "        learning_rate: 学習率\n",
                "        lr_scheduler_type: 学習率スケジューラタイプ\n",
                "        seed: 乱数シード\n",
                "        data_seed: データシード\n",
                "        **kwargs: その他の引数\n",
                "\n",
                "    Returns:\n",
                "        Trainer: 学習済みTrainer\n",
                "    \"\"\"\n",
                "    logger.info(\"Starting training...\")\n",
                "    logger.info(f\"Model: {model.__class__.__name__}\")\n",
                "    logger.info(f\"Dataset: {dataset}\")\n",
                "    logger.info(f\"Output dir: {output_dir}\")\n",
                "\n",
                "    # データコレーターの設定\n",
                "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
                "\n",
                "    # 評価メトリクスの設定\n",
                "    accuracy = evaluate.load(\"accuracy\")\n",
                "\n",
                "    def compute_metrics(eval_pred):\n",
                "        predictions, labels = eval_pred\n",
                "        predictions = np.argmax(predictions, axis=1)\n",
                "        return accuracy.compute(predictions=predictions, references=labels)\n",
                "\n",
                "    # 学習設定\n",
                "    training_args = TrainingArguments(\n",
                "        output_dir=output_dir,\n",
                "        num_train_epochs=num_train_epochs,\n",
                "        per_device_train_batch_size=per_device_train_batch_size,\n",
                "        per_device_eval_batch_size=per_device_eval_batch_size,\n",
                "        warmup_steps=warmup_steps,\n",
                "        weight_decay=weight_decay,\n",
                "        logging_dir=logging_dir,\n",
                "        logging_steps=logging_steps,\n",
                "        eval_strategy=eval_strategy,\n",
                "        save_strategy=save_strategy,\n",
                "        load_best_model_at_end=load_best_model_at_end,\n",
                "        metric_for_best_model=metric_for_best_model,\n",
                "        greater_is_better=greater_is_better,\n",
                "        save_total_limit=save_total_limit,\n",
                "        fp16=fp16,\n",
                "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
                "        learning_rate=learning_rate,\n",
                "        lr_scheduler_type=lr_scheduler_type,\n",
                "        seed=seed,\n",
                "        data_seed=data_seed,\n",
                "        report_to=[],\n",
                "        **kwargs\n",
                "    )\n",
                "\n",
                "    # Trainerの作成\n",
                "    trainer = Trainer(\n",
                "        model=model,\n",
                "        args=training_args,\n",
                "        train_dataset=dataset[\"train\"],\n",
                "        eval_dataset=dataset.get(\"validation\", dataset.get(\"test\")),\n",
                "        processing_class=tokenizer,\n",
                "        data_collator=data_collator,\n",
                "        compute_metrics=compute_metrics,\n",
                "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
                "    )\n",
                "\n",
                "    # 学習実行\n",
                "    trainer.train()\n",
                "\n",
                "    logger.info(\"Training completed\")\n",
                "    return trainer"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. 使用例"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 使用例\n",
                "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
                "from datasets import load_dataset\n",
                "\n",
                "# モデルとトークナイザーの読み込み\n",
                "model_name = \"distilbert-base-uncased\"\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
                "\n",
                "# データセットの読み込み\n",
                "dataset = load_dataset(\"imdb\")\n",
                "\n",
                "# 前処理\n",
                "def preprocess_function(examples):\n",
                "    return tokenizer(examples[\"text\"], truncation=True, padding=True)\n",
                "\n",
                "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
                "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
                "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
                "tokenized_dataset.set_format(\"torch\")\n",
                "\n",
                "# 学習実行\n",
                "trainer = train(\n",
                "    model=model,\n",
                "    tokenizer=tokenizer,\n",
                "    dataset=tokenized_dataset,\n",
                "    output_dir=\"./results\",\n",
                "    num_train_epochs=1,  # デモ用に1エポック\n",
                "    per_device_train_batch_size=8,\n",
                "    per_device_eval_batch_size=8,\n",
                "    logging_steps=100,\n",
                "    save_total_limit=1\n",
                ")\n",
                "\n",
                "print(\"学習完了\")\n",
                "print(f\"ベストモデルのメトリクス: {trainer.state.best_metric}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 学習済みモデルのテスト\n",
                "from transformers import pipeline\n",
                "\n",
                "# 学習したモデルでpipelineを作成\n",
                "classifier = pipeline(\"text-classification\", model=trainer.model, tokenizer=tokenizer)\n",
                "\n",
                "# テストテキスト\n",
                "test_texts = [\n",
                "    \"This movie is fantastic!\",\n",
                "    \"I didn't like this film at all.\",\n",
                "    \"It was an average movie.\"\n",
                "]\n",
                "\n",
                "# 推論実行\n",
                "results = classifier(test_texts)\n",
                "for text, result in zip(test_texts, results):\n",
                "    print(f\"Text: {text}\")\n",
                "    print(f\"Prediction: {result['label']} (confidence: {result['score']:.4f})\")\n",
                "    print()\n"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
