{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# モデル推論\n",
                "\n",
                "学習済みモデルを使用して推論を行います。\n",
                "\n",
                "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. 環境設定\n",
                "\n",
                "まず、GPUが有効になっているか確認します。\n",
                "\n",
                "**設定方法**: メニュー → ランタイム → ランタイムのタイプを変更 → GPU を選択"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# GPU確認\n",
                "import torch\n",
                "print(f\"GPU利用可能: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU名: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. ライブラリのインストール"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q transformers"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. ライブラリのインポート"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from typing import List, Union, Optional, Dict, Any\n",
                "from transformers import (\n",
                "    PreTrainedModel,\n",
                "    PreTrainedTokenizer,\n",
                "    pipeline,\n",
                "    Pipeline\n",
                ")\n",
                "import torch\n",
                "import logging\n",
                "\n",
                "logging.basicConfig(level=logging.INFO)\n",
                "logger = logging.getLogger(__name__)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. ModelInferenceクラスの定義"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ModelInference:\n",
                "    \"\"\"\n",
                "    モデル推論クラス\n",
                "\n",
                "    学習済みモデルを使用して推論を行います。\n",
                "    テキスト分類、トークン分類、質問応答、テキスト生成に対応。\n",
                "\n",
                "    Attributes:\n",
                "        model (PreTrainedModel): 推論対象のモデル\n",
                "        tokenizer (PreTrainedTokenizer): トークナイザー\n",
                "        device (str): 推論デバイス（'cpu' or 'cuda'）\n",
                "        task (str): タスクタイプ\n",
                "        pipeline (Pipeline): Hugging Face pipeline\n",
                "    \"\"\"\n",
                "\n",
                "    def __init__(\n",
                "        self,\n",
                "        model: Optional[PreTrainedModel] = None,\n",
                "        tokenizer: Optional[PreTrainedTokenizer] = None,\n",
                "        model_path: Optional[str] = None,\n",
                "        task: str = \"text-classification\",\n",
                "        device: Optional[str] = None\n",
                "    ):\n",
                "        \"\"\"\n",
                "        Args:\n",
                "            model: 推論対象のモデル\n",
                "            tokenizer: トークナイザー\n",
                "            model_path: モデルパス（modelがNoneの場合）\n",
                "            task: タスクタイプ\n",
                "            device: 推論デバイス\n",
                "        \"\"\"\n",
                "        self.task = task\n",
                "\n",
                "        # デバイスの設定\n",
                "        if device is None:\n",
                "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "        else:\n",
                "            self.device = device\n",
                "\n",
                "        logger.info(f\"Using device: {self.device}\")\n",
                "\n",
                "        # パイプラインの作成\n",
                "        if model_path:\n",
                "            self.pipeline = pipeline(\n",
                "                task=task,\n",
                "                model=model_path,\n",
                "                tokenizer=model_path,\n",
                "                device=0 if self.device == \"cuda\" else -1\n",
                "            )\n",
                "            self.model = self.pipeline.model\n",
                "            self.tokenizer = self.pipeline.tokenizer\n",
                "        elif model and tokenizer:\n",
                "            self.model = model.to(self.device)\n",
                "            self.tokenizer = tokenizer\n",
                "            self.pipeline = pipeline(\n",
                "                task=task,\n",
                "                model=model,\n",
                "                tokenizer=tokenizer,\n",
                "                device=0 if self.device == \"cuda\" else -1\n",
                "            )\n",
                "        else:\n",
                "            raise ValueError(\"model_path または modelとtokenizerの両方を指定してください\")\n",
                "\n",
                "        logger.info(f\"Model loaded for task: {task}\")\n",
                "\n",
                "    def predict(\n",
                "        self,\n",
                "        inputs: Union[str, List[str]],\n",
                "        **kwargs\n",
                "    ) -> Union[Dict[str, Any], List[Dict[str, Any]]]:\n",
                "        \"\"\"\n",
                "        推論を実行\n",
                "\n",
                "        Args:\n",
                "            inputs: 入力テキストまたはテキストのリスト\n",
                "            **kwargs: パイプライン固有の引数\n",
                "\n",
                "        Returns:\n",
                "            推論結果\n",
                "        \"\"\"\n",
                "        logger.info(f\"Running inference on {len(inputs) if isinstance(inputs, list) else 1} input(s)\")\n",
                "\n",
                "        # デフォルトのkwargs設定\n",
                "        default_kwargs = {}\n",
                "        if self.task == \"text-classification\":\n",
                "            default_kwargs.update({\"return_all_scores\": True})\n",
                "        elif self.task == \"text-generation\":\n",
                "            default_kwargs.update({\n",
                "                \"max_length\": 50,\n",
                "                \"num_return_sequences\": 1,\n",
                "                \"temperature\": 0.7\n",
                "            })\n",
                "        elif self.task == \"question-answering\":\n",
                "            # QAの場合はinputsがdictであることを期待\n",
                "            pass\n",
                "\n",
                "        # kwargsのマージ\n",
                "        default_kwargs.update(kwargs)\n",
                "\n",
                "        try:\n",
                "            results = self.pipeline(inputs, **default_kwargs)\n",
                "            logger.info(\"Inference completed successfully\")\n",
                "            return results\n",
                "        except Exception as e:\n",
                "            logger.error(f\"Inference failed: {e}\")\n",
                "            raise\n",
                "\n",
                "    def predict_batch(\n",
                "        self,\n",
                "        inputs: List[str],\n",
                "        batch_size: int = 32,\n",
                "        **kwargs\n",
                "    ) -> List[Dict[str, Any]]:\n",
                "        \"\"\"\n",
                "        バッチ推論を実行\n",
                "\n",
                "        Args:\n",
                "            inputs: 入力テキストのリスト\n",
                "            batch_size: バッチサイズ\n",
                "            **kwargs: パイプライン固有の引数\n",
                "\n",
                "        Returns:\n",
                "            推論結果のリスト\n",
                "        \"\"\"\n",
                "        logger.info(f\"Running batch inference with batch_size={batch_size}\")\n",
                "\n",
                "        results = []\n",
                "        for i in range(0, len(inputs), batch_size):\n",
                "            batch = inputs[i:i + batch_size]\n",
                "            batch_results = self.predict(batch, **kwargs)\n",
                "            results.extend(batch_results)\n",
                "\n",
                "        logger.info(f\"Batch inference completed: {len(results)} results\")\n",
                "        return results\n",
                "\n",
                "    def predict_proba(\n",
                "        self,\n",
                "        inputs: Union[str, List[str]]\n",
                "    ) -> Union[List[float], List[List[float]]]:\n",
                "        \"\"\"\n",
                "        分類タスクの確率を取得\n",
                "\n",
                "        Args:\n",
                "            inputs: 入力テキストまたはテキストのリスト\n",
                "\n",
                "        Returns:\n",
                "            確率のリスト\n",
                "        \"\"\"\n",
                "        if self.task != \"text-classification\":\n",
                "            raise ValueError(\"predict_probaはtext-classificationタスクでのみ使用可能です\")\n",
                "\n",
                "        results = self.predict(inputs, return_all_scores=True)\n",
                "\n",
                "        if isinstance(inputs, str):\n",
                "            # 単一入力\n",
                "            return [score[\"score\"] for score in results]\n",
                "        else:\n",
                "            # 複数入力\n",
                "            return [[score[\"score\"] for score in result] for result in results]\n",
                "\n",
                "    def generate_text(\n",
                "        self,\n",
                "        prompt: str,\n",
                "        max_length: int = 100,\n",
                "        temperature: float = 0.7,\n",
                "        num_return_sequences: int = 1,\n",
                "        **kwargs\n",
                "    ) -> Union[str, List[str]]:\n",
                "        \"\"\"\n",
                "        テキスト生成\n",
                "\n",
                "        Args:\n",
                "            prompt: プロンプト\n",
                "            max_length: 最大生成長\n",
                "            temperature: 温度パラメータ\n",
                "            num_return_sequences: 生成シーケンス数\n",
                "            **kwargs: その他の引数\n",
                "\n",
                "        Returns:\n",
                "            生成されたテキスト\n",
                "        \"\"\"\n",
                "        if self.task != \"text-generation\":\n",
                "            raise ValueError(\"generate_textはtext-generationタスクでのみ使用可能です\")\n",
                "\n",
                "        results = self.predict(\n",
                "            prompt,\n",
                "            max_length=max_length,\n",
                "            temperature=temperature,\n",
                "            num_return_sequences=num_return_sequences,\n",
                "            **kwargs\n",
                "        )\n",
                "\n",
                "        if num_return_sequences == 1:\n",
                "            return results[0][\"generated_text\"]\n",
                "        else:\n",
                "            return [result[\"generated_text\"] for result in results]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. 使用例"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 使用例\n",
                "from transformers import pipeline\n",
                "\n",
                "# 感情分析の例\n",
                "sentiment_analyzer = ModelInference(\n",
                "    model_path=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
                "    task=\"text-classification\"\n",
                ")\n",
                "\n",
                "texts = [\n",
                "    \"I love this product!\",\n",
                "    \"This is terrible.\",\n",
                "    \"It's okay, nothing special.\"\n",
                "]\n",
                "\n",
                "results = sentiment_analyzer.predict(texts)\n",
                "for text, result in zip(texts, results):\n",
                "    print(f\"Text: {text}\")\n",
                "    print(f\"Prediction: {result}\")\n",
                "    print()\n",
                "\n",
                "# テキスト生成の例\n",
                "generator = ModelInference(\n",
                "    model_path=\"gpt2\",\n",
                "    task=\"text-generation\"\n",
                ")\n",
                "prompt = \"The future of AI is\"\n",
                "generated = generator.generate_text(prompt, max_length=50)\n",
                "print(f\"Prompt: {prompt}\")\n",
                "print(f\"Generated: {generated}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 用意したモデルのテスト\n",
                "# models/my-first-model を使用\n",
                "model_path = \"./models/my-first-model\"\n",
                "\n",
                "# ModelInferenceでロード\n",
                "my_model = ModelInference(\n",
                "    model_path=model_path,\n",
                "    task=\"text-classification\"\n",
                ")\n",
                "\n",
                "# テストテキスト\n",
                "test_texts = [\n",
                "    \"This movie is fantastic!\",\n",
                "    \"I didn't like this film at all.\",\n",
                "    \"It was an average movie.\"\n",
                "]\n",
                "\n",
                "# 推論実行\n",
                "results = my_model.predict(test_texts)\n",
                "for text, result in zip(test_texts, results):\n",
                "    print(f\"Text: {text}\")\n",
                "    print(f\"Prediction: {result}\")\n",
                "    print()\n"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
