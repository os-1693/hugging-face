{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 推論スクリプト\n",
                "\n",
                "学習済みモデルを使用してテキスト分類の推論を行います。\n",
                "\n",
                "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. 環境設定\n",
                "\n",
                "まず、GPUが有効になっているか確認します。\n",
                "\n",
                "**設定方法**: メニュー → ランタイム → ランタイムのタイプを変更 → GPU を選択"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# GPU確認\n",
                "import torch\n",
                "print(f\"GPU利用可能: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU名: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. ライブラリのインストール"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q transformers torch"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. ライブラリのインポート"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import argparse\n",
                "import logging\n",
                "\n",
                "import torch\n",
                "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
                "\n",
                "logging.basicConfig(level=logging.INFO)\n",
                "logger = logging.getLogger(__name__)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. ModelInferenceクラスの定義"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ModelInference:\n",
                "    \"\"\"\n",
                "    モデル推論クラス\n",
                "\n",
                "    学習済みモデルをロードし、テキストの分類予測を行います。\n",
                "    単一テキストとバッチ処理の両方に対応しています。\n",
                "\n",
                "    Attributes:\n",
                "        device (str): 使用するデバイス（'cuda' または 'cpu'）\n",
                "        tokenizer: テキストをトークンに変換するトークナイザー\n",
                "        model: 予測に使用する分類モデル\n",
                "    \"\"\"\n",
                "\n",
                "    def __init__(self, model_path: str, device: str = None):\n",
                "        \"\"\"\n",
                "        Args:\n",
                "            model_path: モデルのパス\n",
                "            device: 使用するデバイス（cuda/cpu）\n",
                "        \"\"\"\n",
                "        self.device = device or \"cpu\"  # CPUを使用\n",
                "        logger.info(f\"Loading model from {model_path}\")\n",
                "        logger.info(f\"Using device: {self.device}\")\n",
                "\n",
                "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
                "        self.model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
                "        self.model.to(self.device)\n",
                "        self.model.eval()\n",
                "\n",
                "        # 推論最適化: キャッシュを有効化\n",
                "        if hasattr(self.model, \"config\"):\n",
                "            self.model.config.use_cache = True\n",
                "\n",
                "    def predict(self, text: str) -> dict:\n",
                "        \"\"\"\n",
                "        テキストの予測を行う\n",
                "\n",
                "        Args:\n",
                "            text: 入力テキスト\n",
                "\n",
                "        Returns:\n",
                "            予測結果の辞書\n",
                "        \"\"\"\n",
                "        # トークナイズ\n",
                "        inputs = self.tokenizer(\n",
                "            text, return_tensors=\"pt\", truncation=True, max_length=512, padding=True\n",
                "        )\n",
                "\n",
                "        # デバイスに転送\n",
                "        inputs = {key: value.to(self.device) for key, value in inputs.items()}\n",
                "\n",
                "        # 推論\n",
                "        with torch.no_grad():\n",
                "            outputs = self.model(**inputs)\n",
                "\n",
                "        # 結果の処理\n",
                "        logits = outputs.logits\n",
                "        probabilities = torch.softmax(logits, dim=-1)\n",
                "        predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
                "        confidence = probabilities[0][predicted_class].item()\n",
                "\n",
                "        return {\n",
                "            \"predicted_class\": predicted_class,\n",
                "            \"confidence\": confidence,\n",
                "            \"probabilities\": probabilities[0].cpu().numpy().tolist(),\n",
                "        }\n",
                "\n",
                "    def predict_batch(self, texts: list) -> list:\n",
                "        \"\"\"\n",
                "        複数テキストの予測を行う\n",
                "\n",
                "        Args:\n",
                "            texts: 入力テキストのリスト\n",
                "\n",
                "        Returns:\n",
                "            予測結果のリスト\n",
                "        \"\"\"\n",
                "        # トークナイズ\n",
                "        inputs = self.tokenizer(\n",
                "            texts, return_tensors=\"pt\", truncation=True, max_length=512, padding=True\n",
                "        )\n",
                "\n",
                "        # デバイスに転送\n",
                "        inputs = {key: value.to(self.device) for key, value in inputs.items()}\n",
                "\n",
                "        # 推論\n",
                "        with torch.no_grad():\n",
                "            outputs = self.model(**inputs)\n",
                "\n",
                "        # 結果の処理\n",
                "        logits = outputs.logits\n",
                "        probabilities = torch.softmax(logits, dim=-1)\n",
                "        predicted_classes = torch.argmax(probabilities, dim=-1).cpu().numpy().tolist()\n",
                "        confidences = [\n",
                "            probabilities[i][pred].item() for i, pred in enumerate(predicted_classes)\n",
                "        ]\n",
                "\n",
                "        results = []\n",
                "        for i, (pred_class, confidence) in enumerate(\n",
                "            zip(predicted_classes, confidences)\n",
                "        ):\n",
                "            results.append(\n",
                "                {\n",
                "                    \"text\": texts[i],\n",
                "                    \"predicted_class\": pred_class,\n",
                "                    \"confidence\": confidence,\n",
                "                    \"probabilities\": probabilities[i].cpu().numpy().tolist(),\n",
                "                }\n",
                "            )\n",
                "\n",
                "        return results"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. 使用例"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 使用例\n",
                "# モデルパスの指定（事前に学習済みモデルをアップロードまたはHugging Faceから）\n",
                "# model_path = \"./models/my-model\"  # ローカルパス\n",
                "model_path = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"  # Hugging Faceモデル例\n",
                "\n",
                "# 推論クラスの初期化\n",
                "inference = ModelInference(model_path, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "\n",
                "# 単一テキストの予測\n",
                "text = \"This is great!\"\n",
                "result = inference.predict(text)\n",
                "print(f\"Input: {text}\")\n",
                "print(f\"Predicted class: {result['predicted_class']}\")\n",
                "print(f\"Confidence: {result['confidence']:.4f}\")\n",
                "\n",
                "# バッチ予測\n",
                "texts = [\"This movie was fantastic!\", \"Terrible movie, waste of time.\"]\n",
                "results = inference.predict_batch(texts)\n",
                "for res in results:\n",
                "    print(f\"Text: {res['text']}\")\n",
                "    print(f\"Prediction: {res['predicted_class']} (Confidence: {res['confidence']:.4f})\")\n",
                "    print(\"-\" * 50)"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
