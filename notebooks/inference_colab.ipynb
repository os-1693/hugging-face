{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/os-1693/hugging-face/blob/main/notebooks/inference_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VYJ4y3NF3e3"
      },
      "source": [
        "# モデル推論\n",
        "\n",
        "学習済みモデルを使用して推論を行います。\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89k6pv1VF3e5"
      },
      "source": [
        "## 1. 環境設定\n",
        "\n",
        "まず、GPUが有効になっているか確認します。\n",
        "\n",
        "**設定方法**: メニュー → ランタイム → ランタイムのタイプを変更 → GPU を選択"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "n85Dnr7gF3e6",
        "outputId": "9f0077cb-c089-4f80-cb3e-45c38de35e65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU利用可能: True\n",
            "GPU名: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "# GPU確認\n",
        "import torch\n",
        "print(f\"GPU利用可能: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU名: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gAdD6tAF3e7"
      },
      "source": [
        "## 2. ライブラリのインストール"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "cpu2JR3aF3e7"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2zVyQpvF3e7"
      },
      "source": [
        "## 3. ライブラリのインポート"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "u1wxVl0SF3e8"
      },
      "outputs": [],
      "source": [
        "from typing import List, Union, Optional, Dict, Any\n",
        "from transformers import (\n",
        "    PreTrainedModel,\n",
        "    PreTrainedTokenizer,\n",
        "    pipeline,\n",
        "    Pipeline\n",
        ")\n",
        "import torch\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMsumG5CF3e8"
      },
      "source": [
        "## 4. ModelInferenceクラスの定義"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "tFEeoICMF3e8"
      },
      "outputs": [],
      "source": [
        "class ModelInference:\n",
        "    \"\"\"\n",
        "    モデル推論クラス\n",
        "\n",
        "    学習済みモデルを使用して推論を行います。\n",
        "    テキスト分類、トークン分類、質問応答、テキスト生成に対応。\n",
        "\n",
        "    Attributes:\n",
        "        model (PreTrainedModel): 推論対象のモデル\n",
        "        tokenizer (PreTrainedTokenizer): トークナイザー\n",
        "        device (str): 推論デバイス（'cpu' or 'cuda'）\n",
        "        task (str): タスクタイプ\n",
        "        pipeline (Pipeline): Hugging Face pipeline\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: Optional[PreTrainedModel] = None,\n",
        "        tokenizer: Optional[PreTrainedTokenizer] = None,\n",
        "        model_path: Optional[str] = None,\n",
        "        task: str = \"text-classification\",\n",
        "        device: Optional[str] = None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            model: 推論対象のモデル\n",
        "            tokenizer: トークナイザー\n",
        "            model_path: モデルパス（modelがNoneの場合）\n",
        "            task: タスクタイプ\n",
        "            device: 推論デバイス\n",
        "        \"\"\"\n",
        "        self.task = task\n",
        "\n",
        "        # デバイスの設定\n",
        "        if device is None:\n",
        "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        else:\n",
        "            self.device = device\n",
        "\n",
        "        logger.info(f\"Using device: {self.device}\")\n",
        "\n",
        "        # パイプラインの作成\n",
        "        if model_path:\n",
        "            self.pipeline = pipeline(\n",
        "                task=task,\n",
        "                model=model_path,\n",
        "                tokenizer=model_path,\n",
        "                device=0 if self.device == \"cuda\" else -1\n",
        "            )\n",
        "            self.model = self.pipeline.model\n",
        "            self.tokenizer = self.pipeline.tokenizer\n",
        "        elif model and tokenizer:\n",
        "            self.model = model.to(self.device)\n",
        "            self.tokenizer = tokenizer\n",
        "            self.pipeline = pipeline(\n",
        "                task=task,\n",
        "                model=model,\n",
        "                tokenizer=tokenizer,\n",
        "                device=0 if self.device == \"cuda\" else -1\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(\"model_path または modelとtokenizerの両方を指定してください\")\n",
        "\n",
        "        logger.info(f\"Model loaded for task: {task}\")\n",
        "\n",
        "    def predict(\n",
        "        self,\n",
        "        inputs: Union[str, List[str]],\n",
        "        **kwargs\n",
        "    ) -> Union[Dict[str, Any], List[Dict[str, Any]]]:\n",
        "        \"\"\"\n",
        "        推論を実行\n",
        "\n",
        "        Args:\n",
        "            inputs: 入力テキストまたはテキストのリスト\n",
        "            **kwargs: パイプライン固有の引数\n",
        "\n",
        "        Returns:\n",
        "            推論結果\n",
        "        \"\"\"\n",
        "        logger.info(f\"Running inference on {len(inputs) if isinstance(inputs, list) else 1} input(s)\")\n",
        "\n",
        "        # デフォルトのkwargs設定\n",
        "        default_kwargs = {}\n",
        "        if self.task == \"text-classification\":\n",
        "            default_kwargs.update({\"return_all_scores\": True})\n",
        "        elif self.task == \"text-generation\":\n",
        "            default_kwargs.update({\n",
        "                \"max_new_tokens\": 50,\n",
        "                \"num_return_sequences\": 1,\n",
        "                \"temperature\": 0.7,\n",
        "                \"truncation\": True\n",
        "            })\n",
        "        elif self.task == \"question-answering\":\n",
        "            # QAの場合はinputsがdictであることを期待\n",
        "            pass\n",
        "\n",
        "        # kwargsのマージ\n",
        "        default_kwargs.update(kwargs)\n",
        "\n",
        "        try:\n",
        "            results = self.pipeline(inputs, **default_kwargs)\n",
        "            logger.info(\"Inference completed successfully\")\n",
        "            return results\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Inference failed: {e}\")\n",
        "            raise\n",
        "\n",
        "    def predict_batch(\n",
        "        self,\n",
        "        inputs: List[str],\n",
        "        batch_size: int = 32,\n",
        "        **kwargs\n",
        "    ) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        バッチ推論を実行\n",
        "\n",
        "        Args:\n",
        "            inputs: 入力テキストのリスト\n",
        "            batch_size: バッチサイズ\n",
        "            **kwargs: パイプライン固有の引数\n",
        "\n",
        "        Returns:\n",
        "            推論結果のリスト\n",
        "        \"\"\"\n",
        "        logger.info(f\"Running batch inference with batch_size={batch_size}\")\n",
        "\n",
        "        results = []\n",
        "        for i in range(0, len(inputs), batch_size):\n",
        "            batch = inputs[i:i + batch_size]\n",
        "            batch_results = self.predict(batch, **kwargs)\n",
        "            results.extend(batch_results)\n",
        "\n",
        "        logger.info(f\"Batch inference completed: {len(results)} results\")\n",
        "        return results\n",
        "\n",
        "    def predict_proba(\n",
        "        self,\n",
        "        inputs: Union[str, List[str]]\n",
        "    ) -> Union[List[float], List[List[float]]]:\n",
        "        \"\"\"\n",
        "        分類タスクの確率を取得\n",
        "\n",
        "        Args:\n",
        "            inputs: 入力テキストまたはテキストのリスト\n",
        "\n",
        "        Returns:\n",
        "            確率のリスト\n",
        "        \"\"\"\n",
        "        if self.task != \"text-classification\":\n",
        "            raise ValueError(\"predict_probaはtext-classificationタスクでのみ使用可能です\")\n",
        "\n",
        "        results = self.predict(inputs, return_all_scores=True)\n",
        "\n",
        "        if isinstance(inputs, str):\n",
        "            # 単一入力\n",
        "            return [score[\"score\"] for score in results]\n",
        "        else:\n",
        "            # 複数入力\n",
        "            return [[score[\"score\"] for score in result] for result in results]\n",
        "\n",
        "    def generate_text(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        max_new_tokens: int = 50,\n",
        "        temperature: float = 0.7,\n",
        "        num_return_sequences: int = 1,\n",
        "        **kwargs\n",
        "    ) -> Union[str, List[str]]:\n",
        "        \"\"\"\n",
        "        テキスト生成\n",
        "\n",
        "        Args:\n",
        "            prompt: プロンプト\n",
        "            max_new_tokens: 生成する新しいトークンの最大数\n",
        "            temperature: 温度パラメータ\n",
        "            num_return_sequences: 生成シーケンス数\n",
        "            **kwargs: その他の引数\n",
        "\n",
        "        Returns:\n",
        "            生成されたテキスト\n",
        "        \"\"\"\n",
        "        if self.task != \"text-generation\":\n",
        "            raise ValueError(\"generate_textはtext-generationタスクでのみ使用可能です\")\n",
        "\n",
        "        results = self.predict(\n",
        "            prompt,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=temperature,\n",
        "            num_return_sequences=num_return_sequences,\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "        if num_return_sequences == 1:\n",
        "            return results[0][\"generated_text\"]\n",
        "        else:\n",
        "            return [result[\"generated_text\"] for result in results]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMs4nP44F3e-"
      },
      "source": [
        "## 5. 使用例"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "LACG_M9HF3e-",
        "outputId": "cfdc6c28-4b82-4151-d3d8-f4be8723fb99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cuda:0\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: I love this product!\n",
            "Prediction: [{'label': 'negative', 'score': 0.004656333941966295}, {'label': 'neutral', 'score': 0.01053917407989502}, {'label': 'positive', 'score': 0.9848045110702515}]\n",
            "\n",
            "Text: This is terrible.\n",
            "Prediction: [{'label': 'negative', 'score': 0.9146419763565063}, {'label': 'neutral', 'score': 0.07204394787549973}, {'label': 'positive', 'score': 0.013313977979123592}]\n",
            "\n",
            "Text: It's okay, nothing special.\n",
            "Prediction: [{'label': 'negative', 'score': 0.09523523598909378}, {'label': 'neutral', 'score': 0.5986359119415283}, {'label': 'positive', 'score': 0.30612891912460327}]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: The future of AI is\n",
            "Generated: The future of AI is really going to come down to whether you actually like it or not. You start to see what I mean when I say AI is going to become a reality.\n",
            "\n",
            "Q. What was your reaction when you first announced that you were going to release\n"
          ]
        }
      ],
      "source": [
        "# 使用例\n",
        "from transformers import pipeline\n",
        "\n",
        "# 感情分析の例\n",
        "sentiment_analyzer = ModelInference(\n",
        "    model_path=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
        "    task=\"text-classification\"\n",
        ")\n",
        "\n",
        "texts = [\n",
        "    \"I love this product!\",\n",
        "    \"This is terrible.\",\n",
        "    \"It's okay, nothing special.\"\n",
        "]\n",
        "\n",
        "results = sentiment_analyzer.predict(texts)\n",
        "for text, result in zip(texts, results):\n",
        "    print(f\"Text: {text}\")\n",
        "    print(f\"Prediction: {result}\")\n",
        "    print()\n",
        "\n",
        "# テキスト生成の例\n",
        "generator = ModelInference(\n",
        "    model_path=\"gpt2\",\n",
        "    task=\"text-generation\"\n",
        ")\n",
        "prompt = \"The future of AI is\"\n",
        "generated = generator.generate_text(prompt, max_new_tokens=50)\n",
        "print(f\"Prompt: {prompt}\")\n",
        "print(f\"Generated: {generated}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}