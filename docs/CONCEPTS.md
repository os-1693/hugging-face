# AI学習の基本概念 - 初心者向け解説

このドキュメントでは、AIモデル学習に関する基本的な概念を、専門用語を使わずにわかりやすく説明します。

## 目次

1. [AIモデルとは？](#aiモデルとは)
2. [学習（トレーニング）とは？](#学習トレーニングとは)
3. [重要な用語集](#重要な用語集)
4. [学習の流れ](#学習の流れ)
5. [パラメータの意味](#パラメータの意味)

---

## AIモデルとは？

### 簡単な例え

AIモデルは「勉強する学生」のようなものです。

- **教科書（データセット）** から学ぶ
- **テスト（評価）** で理解度を確認
- **成績（精度）** が向上する

### 具体的には

このプロジェクトでは、**感情分析モデル**を作ります。

```
入力: "This movie is great!"
↓
AIモデルが判断
↓
出力: ポジティブ（92%の確信度）
```

---

## 学習（トレーニング）とは？

### 学習のプロセス

1. **例を見せる**
   ```
   例1: "素晴らしい映画！" → ポジティブ
   例2: "つまらなかった" → ネガティブ
   例3: "最高の体験！" → ポジティブ
   ```

2. **パターンを覚える**
   - 「素晴らしい」「最高」→ ポジティブな言葉
   - 「つまらない」→ ネガティブな言葉

3. **新しい文章で試す**
   ```
   新しい文: "この映画は素晴らしかった！"
   → モデル: "ポジティブだ！"（正解！）
   ```

### 人間の学習との比較

| 人間の学習 | AIの学習 |
|-----------|---------|
| 教科書を読む | データセットを読み込む |
| 例題を解く | 学習データで訓練 |
| テストを受ける | 評価データで性能確認 |
| 間違いから学ぶ | 損失を最小化 |

---

## 重要な用語集

### データセット（Dataset）

**意味**: AIが学習するための「問題集」

**例**:
```
映画レビューデータセット:
- レビュー1: "最高の映画！" → ポジティブ
- レビュー2: "退屈だった" → ネガティブ
- ... (何千件もの例)
```

**種類**:
- **訓練データ（Train）**: 学習に使う（教科書）
- **検証データ（Validation）**: 学習中の確認用（練習問題）
- **テストデータ（Test）**: 最終確認用（本番テスト）

---

### モデル（Model）

**意味**: 実際に予測を行う「AIの脳」

**種類の例**:
- **BERT**: 文章理解が得意（Google開発）
- **DistilBERT**: BERTの軽量版（初心者におすすめ）
- **GPT**: 文章生成が得意（OpenAI開発）

**事前学習モデル**:
- すでに大量のテキストで学習済み
- 少ないデータで fine-tuning（微調整）できる
- 例: 子供に漢字を教えるより、大人に専門用語を教える方が早い

---

### トークナイザー（Tokenizer）

**意味**: テキストを数値に変換するツール

**なぜ必要？**
- コンピュータは文字を直接理解できない
- 数値に変換する必要がある

**例**:
```
入力: "I love AI"
↓ トークナイズ
出力: [101, 1045, 2293, 9932, 102]
     （各単語を数値IDに変換）
```

---

### エポック（Epoch）

**意味**: 全データを1回学習すること

**例**:
```
エポック1: データセット全体を1周学習
エポック2: もう一度データセット全体を学習
エポック3: さらにもう一度...
```

**適切な回数**:
- 少なすぎる（1〜2回）→ 学習不足
- 多すぎる（20回以上）→ 過学習のリスク
- おすすめ: 3〜5回

---

### バッチサイズ（Batch Size）

**意味**: 一度に処理するデータの数

**例え**:
- バッチサイズ1 = 一度に1問ずつ解く
- バッチサイズ32 = 一度に32問まとめて解く

**設定のポイント**:
```
大きい（32, 64）→ 速いがメモリを多く使う
小さい（8, 16）→ 遅いがメモリ節約
```

**初心者におすすめ**: 8 または 16

---

### 学習率（Learning Rate）

**意味**: 学習の「歩幅」の大きさ

**例え**:
- **大きい学習率（0.01）**: 大股で歩く → 速いが不安定
- **小さい学習率（0.00001）**: 小股で歩く → 安定だが遅い
- **適切な学習率（0.00002）**: ちょうど良い

**一般的な値**: `2e-5` (0.00002)

---

### 精度（Accuracy）

**意味**: 正解率（どれだけ正しく予測できたか）

**計算方法**:
```
精度 = 正解した数 ÷ 全体の数

例: 100問中85問正解
→ 85 ÷ 100 = 0.85 = 85%
```

**評価基準**:
- 90%以上: 優秀
- 80〜90%: 良い
- 70〜80%: まずまず
- 70%未満: 改善が必要

---

### 損失（Loss）

**意味**: モデルの「間違い度合い」

**特徴**:
- 小さいほど良い（0に近いほど良い）
- 学習が進むと減少する

**例**:
```
エポック1: Loss = 0.8（まだまだ間違える）
エポック2: Loss = 0.4（改善してきた）
エポック3: Loss = 0.2（かなり良い）
```

---

### Fine-tuning（ファインチューニング）

**意味**: 既存のモデルを特定のタスクに適応させること

**例え**:
```
事前学習モデル: 「英語が読める人」
     ↓ Fine-tuning
あなたのモデル: 「映画レビューの感情分析が得意な人」
```

**メリット**:
- ゼロから学習するより速い
- 少ないデータで高精度
- 計算資源が少なくて済む

---

## 学習の流れ

### 全体の流れ図

```
1. データ準備
   データセットをダウンロード
   ↓
2. モデル読み込み
   事前学習済みモデルを取得
   ↓
3. データ前処理
   テキスト → 数値に変換
   ↓
4. 学習設定
   エポック数、バッチサイズなど
   ↓
5. 学習実行
   モデルを訓練
   ↓
6. 評価
   精度を確認
   ↓
7. 保存
   学習済みモデルを保存
   ↓
8. 推論
   新しいデータで予測
```

### 詳細な各ステップ

#### ステップ1: データ準備
```python
# IMDbデータセットをダウンロード
dataset = load_dataset("imdb")
# 訓練データ: 25,000件の映画レビュー
# テストデータ: 25,000件の映画レビュー
```

#### ステップ2: モデル読み込み
```python
# DistilBERTモデルを読み込み
model = AutoModelForSequenceClassification.from_pretrained(
    "distilbert-base-uncased",
    num_labels=2  # ポジティブ/ネガティブの2クラス
)
```

#### ステップ3: データ前処理
```python
# テキストを数値に変換
tokenized_data = tokenizer(
    "This movie is great!",
    max_length=512  # 最大512トークン
)
# 結果: [101, 2023, 3185, 2003, 2307, 999, 102]
```

#### ステップ4〜5: 学習実行
```python
# Trainerが自動で学習を実行
trainer = Trainer(model=model, args=training_args, ...)
trainer.train()
```

#### ステップ6: 評価
```python
# テストデータで精度を確認
results = trainer.evaluate()
# 結果: Accuracy: 91.5%
```

---

## パラメータの意味

### よく使うパラメータ一覧

| パラメータ | 推奨値（初心者） | 意味 |
|-----------|----------------|------|
| `num_epochs` | 3 | 学習の繰り返し回数 |
| `batch_size` | 8 または 16 | 一度に処理するデータ数 |
| `learning_rate` | 2e-5 | 学習の速度 |
| `max_length` | 256 または 512 | 入力テキストの最大長 |
| `weight_decay` | 0.01 | 過学習を防ぐパラメータ |

### パラメータ調整のコツ

#### メモリ不足エラーが出たら
```yaml
batch_size: 8 → 4 に減らす
max_length: 512 → 256 に減らす
```

#### 精度が低かったら
```yaml
num_epochs: 3 → 5 に増やす
データ量を増やす
```

#### 学習が不安定だったら
```yaml
learning_rate: 2e-5 → 1e-5 に減らす
```

---

## 🎯 まとめ

### 覚えておくべき5つのポイント

1. **データセット**: AIの教科書
2. **モデル**: AIの脳
3. **エポック**: 学習の繰り返し回数（3〜5回がおすすめ）
4. **精度**: 正解率（高いほど良い）
5. **Fine-tuning**: 既存モデルの再利用（効率的！）

### 次のステップ

✅ 実際に `quickstart_simple.py` を実行してみる
✅ パラメータを変更して結果の違いを観察
✅ 自分のデータセットで試してみる

---

## 📚 参考リソース

- [Hugging Face コース（日本語）](https://huggingface.co/learn/nlp-course/ja/chapter1/1)
- [TensorFlowチュートリアル](https://www.tensorflow.org/tutorials?hl=ja)
- [よくある質問](FAQ.md)

---

**質問や困ったことがあれば、README.mdの「トラブルシューティング」セクションをご覧ください。**
