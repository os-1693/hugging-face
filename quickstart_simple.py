"""
ğŸš€ è¶…ç°¡å˜ï¼åˆã‚ã¦ã®AIãƒ¢ãƒ‡ãƒ«å­¦ç¿’

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã€AIå­¦ç¿’ãŒåˆã‚ã¦ã®æ–¹ã§ã‚‚ç°¡å˜ã«å®Ÿè¡Œã§ãã‚‹ã‚ˆã†ã«ä½œã‚‰ã‚Œã¦ã„ã¾ã™ã€‚
å®Ÿè¡Œã™ã‚‹ã ã‘ã§ã€æ„Ÿæƒ…åˆ†æãƒ¢ãƒ‡ãƒ«ï¼ˆãƒã‚¸ãƒ†ã‚£ãƒ–/ãƒã‚¬ãƒ†ã‚£ãƒ–åˆ¤å®šï¼‰ã®å­¦ç¿’ãŒã§ãã¾ã™ã€‚

å®Ÿè¡Œæ–¹æ³•:
    python quickstart_simple.py

æ‰€è¦æ™‚é–“: ç´„3ã€œ5åˆ†ï¼ˆCPUã®å ´åˆã¯10ã€œ15åˆ†ï¼‰
"""

import os
import sys

# =============================================================================
# ğŸ“¦ ã‚¹ãƒ†ãƒƒãƒ—1: å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
# =============================================================================
print("=" * 60)
print("ğŸš€ AIå­¦ç¿’ã‚’å§‹ã‚ã¾ã™ï¼")
print("=" * 60)
print("\nğŸ“¦ ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™...")

try:
    import torch
    from transformers import (
        AutoTokenizer,
        AutoModelForSequenceClassification,
        TrainingArguments,
        Trainer
    )
    from datasets import load_dataset
    import numpy as np
    print("âœ“ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
except ImportError as e:
    print(f"âŒ ã‚¨ãƒ©ãƒ¼: å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
    print(f"   ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„:")
    print(f"   pip install -r requirements.txt")
    sys.exit(1)

# =============================================================================
# ğŸ–¥ï¸  ã‚¹ãƒ†ãƒƒãƒ—2: ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒã‚¤ã‚¹ï¼ˆGPU or CPUï¼‰ã®ç¢ºèª
# =============================================================================
print("\nğŸ–¥ï¸  ã‚¹ãƒ†ãƒƒãƒ—2: ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒã‚¤ã‚¹ã‚’ç¢ºèªã—ã¦ã„ã¾ã™...")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"âœ“ ãƒ‡ãƒã‚¤ã‚¹: {device}")

if torch.cuda.is_available():
    print(f"  GPUå: {torch.cuda.get_device_name(0)}")
    print(f"  ğŸ’¡ GPUãŒä½¿ãˆã‚‹ã®ã§ã€å­¦ç¿’ã¯é€Ÿãå®Œäº†ã—ã¾ã™ï¼")
else:
    print(f"  ğŸ’¡ CPUã§å®Ÿè¡Œã—ã¾ã™ï¼ˆGPUã‚ˆã‚Šæ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ãŒå•é¡Œã‚ã‚Šã¾ã›ã‚“ï¼‰")

# =============================================================================
# ğŸ“š ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™
# =============================================================================
print("\nğŸ“š ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ã„ã¾ã™...")
print("   ï¼ˆåˆå›ã®ã¿æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼‰")

try:
    # IMDbã®æ˜ ç”»ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆæ„Ÿæƒ…åˆ†æç”¨ï¼‰
    # ãƒã‚¸ãƒ†ã‚£ãƒ–ãªãƒ¬ãƒ“ãƒ¥ãƒ¼ã¨ãƒã‚¬ãƒ†ã‚£ãƒ–ãªãƒ¬ãƒ“ãƒ¥ãƒ¼ãŒå«ã¾ã‚Œã¦ã„ã¾ã™
    # åˆå¿ƒè€…å‘ã‘ã«å°‘é‡ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿ä½¿ç”¨ï¼ˆå­¦ç¿’æ™‚é–“ã‚’çŸ­ç¸®ï¼‰
    dataset = load_dataset(
        "imdb",
        split={
            "train": "train[:1000]",  # è¨“ç·´ç”¨: 1000ä»¶ã®ã¿ä½¿ç”¨
            "test": "test[:200]"       # ãƒ†ã‚¹ãƒˆç”¨: 200ä»¶ã®ã¿ä½¿ç”¨
        }
    )
    print(f"âœ“ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†ï¼")
    print(f"  - è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(dataset['train'])}ä»¶")
    print(f"  - ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(dataset['test'])}ä»¶")

    # ãƒ‡ãƒ¼ã‚¿ã®ä¾‹ã‚’è¡¨ç¤º
    print(f"\nğŸ“ ãƒ‡ãƒ¼ã‚¿ã®ä¾‹:")
    example = dataset['train'][0]
    print(f"  ãƒ¬ãƒ“ãƒ¥ãƒ¼: {example['text'][:100]}...")
    print(f"  æ„Ÿæƒ…: {'ãƒã‚¸ãƒ†ã‚£ãƒ–' if example['label'] == 1 else 'ãƒã‚¬ãƒ†ã‚£ãƒ–'}")

except Exception as e:
    print(f"âŒ ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ")
    print(f"   ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šã‚’ç¢ºèªã—ã¦ãã ã•ã„")
    print(f"   ã‚¨ãƒ©ãƒ¼è©³ç´°: {e}")
    sys.exit(1)

# =============================================================================
# ğŸ¤– ã‚¹ãƒ†ãƒƒãƒ—4: AIãƒ¢ãƒ‡ãƒ«ã®æº–å‚™
# =============================================================================
print("\nğŸ¤– ã‚¹ãƒ†ãƒƒãƒ—4: AIãƒ¢ãƒ‡ãƒ«ã‚’æº–å‚™ã—ã¦ã„ã¾ã™...")

# ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«: DistilBERTï¼ˆBERTã®è»½é‡ç‰ˆã§åˆå¿ƒè€…ã«ãŠã™ã™ã‚ï¼‰
model_name = "distilbert-base-uncased"
print(f"   ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {model_name}")

try:
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼: ãƒ†ã‚­ã‚¹ãƒˆã‚’AIãŒç†è§£ã§ãã‚‹æ•°å€¤ã«å¤‰æ›ã™ã‚‹ãƒ„ãƒ¼ãƒ«
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    print(f"âœ“ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿å®Œäº†")

    # ãƒ¢ãƒ‡ãƒ«: å®Ÿéš›ã«æ„Ÿæƒ…åˆ†æã‚’è¡Œã†AI
    # num_labels=2 ã¯ã€Œãƒã‚¸ãƒ†ã‚£ãƒ–ã€ã¨ã€Œãƒã‚¬ãƒ†ã‚£ãƒ–ã€ã®2ã‚¯ãƒ©ã‚¹
    model = AutoModelForSequenceClassification.from_pretrained(
        model_name,
        num_labels=2
    )
    model.to(device)
    print(f"âœ“ ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿å®Œäº†")

except Exception as e:
    print(f"âŒ ã‚¨ãƒ©ãƒ¼: ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
    print(f"   ã‚¨ãƒ©ãƒ¼è©³ç´°: {e}")
    sys.exit(1)

# =============================================================================
# ğŸ”§ ã‚¹ãƒ†ãƒƒãƒ—5: ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†
# =============================================================================
print("\nğŸ”§ ã‚¹ãƒ†ãƒƒãƒ—5: ãƒ‡ãƒ¼ã‚¿ã‚’å‰å‡¦ç†ã—ã¦ã„ã¾ã™...")

def preprocess_function(examples):
    """
    ãƒ†ã‚­ã‚¹ãƒˆã‚’AIãŒç†è§£ã§ãã‚‹å½¢å¼ã«å¤‰æ›ã™ã‚‹é–¢æ•°

    ä¾‹: "This movie is great!" â†’ [101, 2023, 3185, 2003, 2307, 999, 102]
    """
    return tokenizer(
        examples['text'],
        padding="max_length",  # å…¨ã¦åŒã˜é•·ã•ã«æƒãˆã‚‹
        truncation=True,       # é•·ã™ãã‚‹æ–‡ç« ã¯åˆ‡ã‚Šè©°ã‚ã‚‹
        max_length=256         # æœ€å¤§256ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆåˆå¿ƒè€…å‘ã‘ã«çŸ­ã‚ã«è¨­å®šï¼‰
    )

try:
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã«å‰å‡¦ç†ã‚’é©ç”¨
    tokenized_dataset = dataset.map(
        preprocess_function,
        batched=True,
        remove_columns=['text']  # å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã¯ä¸è¦ãªã®ã§å‰Šé™¤
    )
    print(f"âœ“ ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ")

except Exception as e:
    print(f"âŒ ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ")
    print(f"   ã‚¨ãƒ©ãƒ¼è©³ç´°: {e}")
    sys.exit(1)

# =============================================================================
# ğŸ“Š ã‚¹ãƒ†ãƒƒãƒ—6: è©•ä¾¡æŒ‡æ¨™ã®è¨­å®š
# =============================================================================
print("\nğŸ“Š ã‚¹ãƒ†ãƒƒãƒ—6: è©•ä¾¡æŒ‡æ¨™ã‚’è¨­å®šã—ã¦ã„ã¾ã™...")

def compute_metrics(eval_pred):
    """
    ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’è©•ä¾¡ã™ã‚‹é–¢æ•°
    æ­£è§£ç‡ï¼ˆAccuracyï¼‰ã‚’è¨ˆç®—ã—ã¾ã™
    """
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    accuracy = (predictions == labels).mean()
    return {"accuracy": accuracy}

print(f"âœ“ è©•ä¾¡æŒ‡æ¨™ã®è¨­å®šå®Œäº†")

# =============================================================================
# ğŸ¯ ã‚¹ãƒ†ãƒƒãƒ—7: å­¦ç¿’ã®è¨­å®š
# =============================================================================
print("\nğŸ¯ ã‚¹ãƒ†ãƒƒãƒ—7: å­¦ç¿’ã®è¨­å®šã‚’ã—ã¦ã„ã¾ã™...")

# å­¦ç¿’çµæœã®ä¿å­˜å…ˆ
output_dir = "./models/my-first-model"
os.makedirs(output_dir, exist_ok=True)

# å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¨­å®š
training_args = TrainingArguments(
    output_dir=output_dir,
    num_train_epochs=2,              # å­¦ç¿’ã®ç¹°ã‚Šè¿”ã—å›æ•°ï¼ˆ2å›ï¼ç´„5åˆ†ï¼‰
    per_device_train_batch_size=8,   # ä¸€åº¦ã«å‡¦ç†ã™ã‚‹ãƒ‡ãƒ¼ã‚¿æ•°ï¼ˆå°ã•ã‚ã§å®‰å…¨ï¼‰
    per_device_eval_batch_size=16,   # è©•ä¾¡æ™‚ã®å‡¦ç†æ•°
    learning_rate=2e-5,              # å­¦ç¿’é€Ÿåº¦ï¼ˆã“ã®å€¤ãŒä¸€èˆ¬çš„ï¼‰
    weight_decay=0.01,               # éå­¦ç¿’ã‚’é˜²ããƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    evaluation_strategy="epoch",     # å„ã‚¨ãƒãƒƒã‚¯å¾Œã«è©•ä¾¡
    save_strategy="epoch",           # å„ã‚¨ãƒãƒƒã‚¯å¾Œã«ä¿å­˜
    logging_dir='./logs',            # ãƒ­ã‚°ã®ä¿å­˜å…ˆ
    logging_steps=50,                # 50ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«ãƒ­ã‚°å‡ºåŠ›
    load_best_model_at_end=True,     # æœ€ã‚‚è‰¯ã„ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
    report_to=["tensorboard"],       # TensorBoardã§å¯è¦–åŒ–
)

print(f"âœ“ å­¦ç¿’è¨­å®šå®Œäº†")
print(f"  - ã‚¨ãƒãƒƒã‚¯æ•°: 2å›")
print(f"  - ãƒãƒƒãƒã‚µã‚¤ã‚º: 8")
print(f"  - ä¿å­˜å…ˆ: {output_dir}")

# =============================================================================
# ğŸ‹ï¸ ã‚¹ãƒ†ãƒƒãƒ—8: å­¦ç¿’ã®é–‹å§‹ï¼
# =============================================================================
print("\n" + "=" * 60)
print("ğŸ‹ï¸  ã‚¹ãƒ†ãƒƒãƒ—8: å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™ï¼")
print("=" * 60)
print("â° äºˆæƒ³æ™‚é–“: 3ã€œ5åˆ†ï¼ˆCPUã®å ´åˆã¯10ã€œ15åˆ†ï¼‰")
print("ğŸ’¡ å­¦ç¿’ä¸­ã¯ã‚³ãƒ¼ãƒ’ãƒ¼ã§ã‚‚é£²ã‚“ã§ãŠå¾…ã¡ãã ã•ã„â˜•")
print()

try:
    # Trainer: å­¦ç¿’ã‚’è‡ªå‹•ã§è¡Œã£ã¦ãã‚Œã‚‹ä¾¿åˆ©ãªãƒ„ãƒ¼ãƒ«
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset['train'],
        eval_dataset=tokenized_dataset['test'],
        compute_metrics=compute_metrics,
    )

    # å­¦ç¿’é–‹å§‹ï¼
    train_result = trainer.train()

    print("\n" + "=" * 60)
    print("ğŸ‰ å­¦ç¿’ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    print("=" * 60)

except Exception as e:
    print(f"\nâŒ ã‚¨ãƒ©ãƒ¼: å­¦ç¿’ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
    print(f"   ã‚¨ãƒ©ãƒ¼è©³ç´°: {e}")
    sys.exit(1)

# =============================================================================
# ğŸ’¾ ã‚¹ãƒ†ãƒƒãƒ—9: ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
# =============================================================================
print("\nğŸ’¾ ã‚¹ãƒ†ãƒƒãƒ—9: ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¦ã„ã¾ã™...")

try:
    trainer.save_model()
    tokenizer.save_pretrained(output_dir)
    print(f"âœ“ ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜å®Œäº†: {output_dir}")

except Exception as e:
    print(f"âŒ ã‚¨ãƒ©ãƒ¼: ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ")
    print(f"   ã‚¨ãƒ©ãƒ¼è©³ç´°: {e}")

# =============================================================================
# ğŸ“ˆ ã‚¹ãƒ†ãƒƒãƒ—10: çµæœã®ç¢ºèª
# =============================================================================
print("\nğŸ“ˆ ã‚¹ãƒ†ãƒƒãƒ—10: çµæœã‚’ç¢ºèªã—ã¦ã„ã¾ã™...")

try:
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡
    eval_result = trainer.evaluate()

    print("\n" + "=" * 60)
    print("ğŸ“Š æœ€çµ‚çµæœ")
    print("=" * 60)
    print(f"æ­£è§£ç‡ï¼ˆAccuracyï¼‰: {eval_result['eval_accuracy']:.2%}")
    print(f"æå¤±ï¼ˆLossï¼‰: {eval_result['eval_loss']:.4f}")
    print()

    # çµæœã®è§£é‡ˆ
    accuracy = eval_result['eval_accuracy']
    if accuracy >= 0.85:
        print("ğŸŒŸ ç´ æ™´ã‚‰ã—ã„ï¼éå¸¸ã«é«˜ã„ç²¾åº¦ã§ã™ï¼")
    elif accuracy >= 0.75:
        print("ğŸ‘ è‰¯ã„çµæœã§ã™ï¼")
    elif accuracy >= 0.65:
        print("ğŸ“ ã¾ãšã¾ãšã®çµæœã§ã™ã€‚ã‚¨ãƒãƒƒã‚¯æ•°ã‚’å¢—ã‚„ã™ã¨æ”¹å–„ã™ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚")
    else:
        print("ğŸ’¡ ã‚‚ã†å°‘ã—æ”¹å–„ã®ä½™åœ°ãŒã‚ã‚Šãã†ã§ã™ã€‚")

except Exception as e:
    print(f"âŒ ã‚¨ãƒ©ãƒ¼: è©•ä¾¡ã«å¤±æ•—ã—ã¾ã—ãŸ")
    print(f"   ã‚¨ãƒ©ãƒ¼è©³ç´°: {e}")

# =============================================================================
# ğŸ§ª ã‚¹ãƒ†ãƒƒãƒ—11: å®Ÿéš›ã«ä½¿ã£ã¦ã¿ã‚ˆã†ï¼
# =============================================================================
print("\n" + "=" * 60)
print("ğŸ§ª ã‚¹ãƒ†ãƒƒãƒ—11: ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿéš›ã«ä½¿ã£ã¦ã¿ã¾ã—ã‚‡ã†ï¼")
print("=" * 60)

# ãƒ†ã‚¹ãƒˆç”¨ã®ã‚µãƒ³ãƒ—ãƒ«æ–‡ç« 
test_texts = [
    "This movie was absolutely fantastic! I loved every minute of it.",
    "Terrible movie. Complete waste of time and money.",
    "It was okay, nothing special but not bad either."
]

print("\näºˆæ¸¬ã‚’å®Ÿè¡Œä¸­...\n")

for i, text in enumerate(test_texts, 1):
    # ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=256)
    inputs = {key: value.to(device) for key, value in inputs.items()}

    # äºˆæ¸¬
    model.eval()
    with torch.no_grad():
        outputs = model(**inputs)
        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)

    # çµæœã‚’è¡¨ç¤º
    negative_prob = predictions[0][0].item()
    positive_prob = predictions[0][1].item()
    sentiment = "ãƒã‚¸ãƒ†ã‚£ãƒ– ğŸ˜Š" if positive_prob > negative_prob else "ãƒã‚¬ãƒ†ã‚£ãƒ– ğŸ˜”"
    confidence = max(negative_prob, positive_prob)

    print(f"ä¾‹ {i}:")
    print(f"  å…¥åŠ›: {text}")
    print(f"  äºˆæ¸¬: {sentiment}")
    print(f"  ç¢ºä¿¡åº¦: {confidence:.1%}")
    print(f"  è©³ç´°: ãƒã‚¬ãƒ†ã‚£ãƒ– {negative_prob:.1%} | ãƒã‚¸ãƒ†ã‚£ãƒ– {positive_prob:.1%}")
    print()

# =============================================================================
# âœ… å®Œäº†ï¼
# =============================================================================
print("=" * 60)
print("âœ… ã™ã¹ã¦å®Œäº†ã—ã¾ã—ãŸï¼ãŠã‚ã§ã¨ã†ã”ã–ã„ã¾ã™ï¼ğŸ‰")
print("=" * 60)
print()
print("ğŸ“ å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã®å ´æ‰€:")
print(f"   {output_dir}")
print()
print("ğŸ” æ¬¡ã«ã‚„ã£ã¦ã¿ã‚‹ã“ã¨:")
print("   1. TensorBoardã§å­¦ç¿’ã®æ§˜å­ã‚’ç¢ºèª:")
print("      tensorboard --logdir ./logs")
print()
print("   2. è‡ªåˆ†ã®ãƒ†ã‚­ã‚¹ãƒˆã§äºˆæ¸¬ã—ã¦ã¿ã‚‹:")
print(f"      python src/inference.py --model_path {output_dir} \\")
print('        --text "Your text here"')
print()
print("   3. ã‚ˆã‚Šé•·ãå­¦ç¿’ã—ã¦ã¿ã‚‹:")
print("      config/beginner_config.yaml ã® num_epochs ã‚’ 5 ã«å¤‰æ›´")
print()
print("ğŸ“š è©³ã—ã„æƒ…å ±:")
print("   - ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆã‚¬ã‚¤ãƒ‰: QUICKSTART.md")
print("   - åŸºæœ¬æ¦‚å¿µã®èª¬æ˜: docs/CONCEPTS.md")
print("=" * 60)
